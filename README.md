ğŸ“˜ JurisRAG
AI-Powered Legal Information Retrieval System
âš–ï¸ Overview

JurisRAG is an AI-powered legal information retrieval system built using Retrieval-Augmented Generation (RAG).
It enables users to query statutory documents and receive accurate, context-grounded responses.

Instead of generating answers purely from a language modelâ€™s internal knowledge, JurisRAG retrieves relevant sections from legal documents and then generates responses grounded in that retrieved context.

This significantly reduces hallucination and improves reliability for legal domain queries.

ğŸš€ Key Features

ğŸ“„ PDF-based statutory corpus ingestion

ğŸ” Semantic search using FAISS vector indexing

ğŸ§  Context-aware response generation (RAG architecture)

âš¡ Fast nearest-neighbor retrieval

ğŸ’¬ Clean Streamlit-based interactive UI

ğŸ” Environment-variable based API key management

ğŸ—ï¸ System Architecture
1ï¸âƒ£ Data Ingestion & Indexing Pipeline
PDF Loader 
   â†“
Text Extraction 
   â†“
Sliding-Window Chunking 
   â†“
Embedding Generation (MiniLM) 
   â†“
FAISS Vector Store
2ï¸âƒ£ Query Processing Flow
User Query
   â†“
Query Embedding
   â†“
Top-K Semantic Retrieval
   â†“
LLM Response Generation (Groq - Llama)
   â†“
Final Context-Grounded Answer
ğŸ› ï¸ Technologies Used
ğŸ”¹ Backend & RAG

LangChain

FAISS (Vector Database)

HuggingFace Sentence Transformers

Groq (Llama Model API)

ğŸ”¹ Frontend

Streamlit

ğŸ”¹ Supporting Libraries

PyPDF (PDF parsing)

Python-dotenv (environment management)

NumPy

Transformers

Torch

ğŸ“š Legal Corpus Used

The system indexes statutory documents including:

Copyright Act

Criminal Law Amendment Act 2018

Indian Penal Code

Companies Act 2013

Constitution of India

Other legal documents (PDF-based corpus)

âš™ï¸ Installation Guide
1ï¸âƒ£ Clone the Repository
git clone <your-repo-link>
cd JurisRAG
2ï¸âƒ£ Install Dependencies
pip install -r requirements.txt
3ï¸âƒ£ Set API Key

Create a .env file:

GROQ_API_KEY=your_api_key_here
4ï¸âƒ£ Run Ingestion Pipeline
python ingestion.py

This creates the FAISS vector store.

5ï¸âƒ£ Run Application
streamlit run app.py
ğŸ“‚ Project Structure
JurisRAG/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ ingestion.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ my_vector_store/
â””â”€â”€ LEGAL-DATA/
ğŸ¯ How It Works

Legal PDFs are loaded and parsed.

Text is split using sliding-window chunking.

Each chunk is converted into embeddings using MiniLM.

Embeddings are stored in FAISS.

When a query is asked:

The query is embedded

Top relevant chunks are retrieved

Retrieved context is passed to the LLM

A grounded answer is generated

ğŸ§  Why RAG Instead of Pure LLM?

Traditional LLMs:

May hallucinate

Lack document grounding

RAG-based systems:

Retrieve actual legal text

Reduce hallucination

Improve factual accuracy

Provide explainable answers

ğŸ“Œ Limitations

Depends on quality of indexed documents

Does not replace professional legal advice

Accuracy depends on chunking and retrieval strategy

ğŸ”® Future Improvements

ğŸ“¤ User-uploaded document support

ğŸ’¾ Save chat history feature

ğŸ“Š Confidence scoring for retrieved answers

ğŸ” Section-wise citation highlighting

ğŸŒ Deployment on cloud infrastructure

ğŸ‘¨â€ğŸ’» Authors

Harsh Balkrishna Vahal

Project Guide: Prof. Pallavi Nikumbh

Department of Computer Science
PCCOE

ğŸ“œ License

This project is developed for academic and research purposes.
